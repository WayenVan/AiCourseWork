{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "\n",
    "# Let's import basic tools for defining the function and doing the gradient-based learning\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "#from sklearn.preprocessing import PolynomialFeatures # you can try with polynomial basis if you want (It is difficult!)\n",
    "from sklearn.linear_model import SGDRegressor # this defines the SGD function\n",
    "from sklearn.kernel_approximation import RBFSampler # this is the RBF function transformation method\n",
    "\n",
    "from scipy.linalg import norm, pinv\n",
    "\n",
    "#import environment\n",
    "import sys\n",
    "sys.path.append(r'../virl')\n",
    "import virl\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Policy function\n",
    "\"\"\"\n",
    "\n",
    "def create_policy(func_approximator, epsilon, nA):\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = func_approximator.predict(state)\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A,q_values  # return the potentially stochastic policy (which is due to the exploration)\n",
    "\n",
    "    return policy_fn # return a handle to the function so we can call it in the future\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Execute the policy\n",
    "\"\"\"\n",
    "def exec_policy(env, func_approximator, verbose=False):\n",
    "    \"\"\"\n",
    "        A function for executing a policy given the funciton\n",
    "        approximation (the exploration is zero)\n",
    "    \"\"\"\n",
    "\n",
    "    # The policy is defined by our function approximator (of the utility)... let's get a hdnle to that function\n",
    "    policy = create_policy(func_approximator, 0.0, env.action_space.n)\n",
    "            \n",
    "    # Reset the environment and pick the first action\n",
    "    state = env.reset()\n",
    "                    \n",
    "    # One step in the environment\n",
    "    for t in itertools.count():\n",
    "        env.render()\n",
    "\n",
    "        # The policy is stochastic due to exploration \n",
    "        # i.e. the policy recommends not only one action but defines a \n",
    "        # distrbution , \\pi(a|s)\n",
    "        pi_action_state, q_values = policy(state)\n",
    "        action = np.random.choice(np.arange(len(pi_action_state)), p=pi_action_state)\n",
    "        #print(\"Action (%s): %s\" % (action_probs,action)\n",
    "\n",
    "        # Execute action and observe where we end up incl reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Step %d/199:\\n\" % (t), end=\"\")\n",
    "            print(\"\\t state     : %s\\n\" % (state), end=\"\")            \n",
    "            print(\"\\t q_approx  : %s\\n\" % (q_values.T), end=\"\")\n",
    "            print(\"\\t pi(a|s)   : %s\\n\" % (pi_action_state), end=\"\")            \n",
    "            print(\"\\t action    : %s\\n\" % (action), end=\"\")\n",
    "            print(\"\\t next_state: %s\\n\" % (next_state), end=\"\")\n",
    "            print(\"\\t reward    : %s\\n\" % (reward), end=\"\")                        \n",
    "        else:\n",
    "            print(\"\\rStep {}\".format(t), end=\"\")\n",
    "       \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Function Approximation\n",
    "\"\"\"\n",
    "\n",
    "class FunctionApproximator():\n",
    "    \"\"\"\n",
    "    Q(s,a) function approximator. \n",
    "\n",
    "    it uses a specific form for Q(s,a) where seperate functions are fitteted for each \n",
    "    action (i.e. four Q_a(s) individual functions)\n",
    "\n",
    "    We could have concatenated the feature maps with the action TODO TASK?\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, eta0= 0.01, learning_rate= \"constant\"):\n",
    "      \n",
    "        self.eta0=eta0\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "        self.models = []\n",
    "        for _ in range(env.action_space.n):\n",
    "\n",
    "            model = SGDRegressor(learning_rate=learning_rate, tol=1e-5, max_iter=1e5, eta0=eta0)\n",
    "            \n",
    "            model.partial_fit([self.featurize_state(env.reset())], [0])\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def featurize_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"\n",
    "        s_scaled = scaler.transform([state])\n",
    "        s_transformed = feature_transformer.transform(s_scaled)\n",
    "        return s_transformed[0]\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Makes Q(s,a) function predictions.\n",
    "        \n",
    "        Args:\n",
    "            s: state to make a prediction for\n",
    "            a: (Optional) action to make a prediction for\n",
    "            \n",
    "        Returns\n",
    "            If an action a is given this returns a single number as the prediction.\n",
    "            If no action is given this returns a vector or predictions for all actions\n",
    "            in the environment where pred[i] is the prediction for action i.\n",
    "            \n",
    "        \"\"\"\n",
    "        features = self.featurize_state(s)\n",
    "        if a==None:\n",
    "            return np.array([m.predict([features])[0] for m in self.models])\n",
    "        else:            \n",
    "            return self.models[a].predict([features])[0]\n",
    "    \n",
    "    def update(self, s, a, td_target):\n",
    "        \"\"\"\n",
    "        Updates the approximator's parameters (i.e. the weights) for a given state and action towards\n",
    "        the target y (which is the TD target).\n",
    "        \"\"\"\n",
    "        features = self.featurize_state(s)\n",
    "        self.models[a].partial_fit([features], [td_target]) # recall that we have a seperate funciton for each a \n",
    "    \n",
    "    def new_episode(self):        \n",
    "        self.t_episode  = 0.  \n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Reinforce learning\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reinforce(env, func_approximator, num_episodes, discount_factor=1.0, epsilon=0.01, epsilon_decay=1.0):\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Algorithm. Optimizes the policy\n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized         \n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: reward discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \n",
    "    Adapted from: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        policy = create_policy(\n",
    "            func_approximator, epsilon * epsilon_decay**i_episode, env.action_space.n)\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step                                   \n",
    "            action_probs, q_vals = policy(state)\n",
    "            \n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            ##\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode, step-by-step and make policy updates (note we sometime use j for the individual steps)\n",
    "        func_approximator.new_episode()\n",
    "        new_theta=[]\n",
    "        for t, transition in enumerate(episode):                 \n",
    "            # The return, G_t, after this timestep; this is the target for the PolicyEstimator\n",
    "            G_t = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "           \n",
    "            # Update our policy estimator\n",
    "            func_approximator.update(transition.state, transition.action,np.array(G_t))            \n",
    "         \n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "# Load the file\n",
    "env = virl.Epidemic(problem_id = 0, noisy = False)\n",
    "\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "feature_transformer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "feature_transformer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "my_func_approximator = FunctionApproximator()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
