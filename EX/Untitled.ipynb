{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'virl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4796e05c8712>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvirl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'virl'"
     ]
    }
   ],
   "source": [
    "import virl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class SARSA:\n",
    "    '''\n",
    "    Agent class for poliyc search method Sarsa, which is a tabular based reinforcement learning metho\n",
    "    '''\n",
    "    def __init__(self, actions, lr=0.01, reward_decay_factor=0.95, eps=0.8):\n",
    "        self.actions = actions  # 动作空间 action space\n",
    "        self.lr = lr # 学习率 learning rate\n",
    "        self.gamma = reward_decay_factor # 奖励折扣系数 reward factor\n",
    "        self.epsilon = eps # 初始eps值，用于e-greedy探索方法\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) # 存储Q表 storing Q table\n",
    "\n",
    "    '''Choose the next action to take given the observed state using an epsilon greedy policy'''\n",
    "    def select_action(self, observation):\n",
    "        # 检查是否该state是否存在，不存在就插入\n",
    "        self.check_state_exist(observation)\n",
    " \n",
    "        if np.random.uniform() >= self.epsilon:\n",
    "            # 选取Q值最大的动作\n",
    "            state_action_values = self.q_table.loc[observation, :]\n",
    "            action = np.random.choice(state_action_values[state_action_values == np.max(state_action_values)].index) # handle multiple argmax with random\n",
    "        else:\n",
    "            # 随机选取动作\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_, a_):\n",
    "        self.check_state_exist(s_)\n",
    "\n",
    "        if s_ != 'done':\n",
    "            a_ = self.select_action(str(s_)) # argmax action\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, a_] # max state-action value\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "\n",
    "        self.q_table.loc[s, a] = self.q_table.loc[s, a] + self.lr * (q_target - self.q_table.loc[s, a])\n",
    "\n",
    "        self.epsilon = max(0.2, self.epsilon*0.995) # 更新 epsilon的值，希望训练得到后期随机选动作的概率变小\n",
    "\n",
    "        return s_, a_\n",
    "\n",
    "    # 动态添加Q表记录\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # 如果当前state不在Q表中，插入该记录 insert the record if the current state is not in Q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# 原本的观测状态空间过大，因此做一个状态空间压缩，先算出每种人数的所占比例，然后放大100倍。使得每个state都在[0,100]范围内\n",
    "# The original massive observation space make the problem unsolvable with tabular method.\n",
    "def discretized(state, N):\n",
    "    new_state = state/N*100 # 按比例压缩\n",
    "    new_state = np.round(new_state).astype(int)\n",
    "    return new_state\n",
    "\n",
    "# 训练函数\n",
    "def train(env, agent, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        s = env.reset() # 重置环境 开始训练 reset the env to start new epoch of training\n",
    "        done = False\n",
    "        while not done:\n",
    "          # action = np.random.choice(env.action_space.n)\n",
    "            d_s = discretized(s, env.N) # 获得离散后的观测状态 get the processed state\n",
    "            action = agent.select_action(str(d_s)) # 选取当前动作\n",
    "            # print(action)\n",
    "            s_next, reward, done, i = env.step(action=action) # 执行动作 execute an action\n",
    "\n",
    "            d_s_next = discretized(s_next, env.N) # 获得离散后的观测状态\n",
    "            action_next = agent.select_action(str(d_s_next)) # 根据当前策略预测下一个状态的动作\n",
    "\n",
    "            if done:\n",
    "                d_s_next = 'done'\n",
    "            agent.learn(str(d_s), action, reward, str(d_s_next), action_next) # 学习经验\n",
    "\n",
    "            s = s_next\n",
    "            action = action_next\n",
    "\n",
    "\n",
    "# 评估函数\n",
    "def evaluation(env, agent):\n",
    "    states = [] # 存储状态\n",
    "    rewards = [] # 存储奖励值\n",
    "\n",
    "    s = env.reset() # 重置环境\n",
    "    while True:\n",
    "        d_s = discretized(s, env.N) # 获得离散后的观测状态\n",
    "        action = agent.select_action(str(d_s)) # 选取当前动作\n",
    "\n",
    "        s, reward, done, i = env.step(action=action) # 执行动作\n",
    "\n",
    "        states.append(s)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # 可视化结果\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(states)\n",
    "    for i in range(4):\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "    axes[0].set_xlabel('weeks since start of epidemic')\n",
    "    axes[0].set_ylabel('State s(t)')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(rewards);\n",
    "    axes[1].set_title('Reward')\n",
    "    axes[1].set_xlabel('weeks since start of epidemic')\n",
    "    axes[1].set_ylabel('reward r(t)')\n",
    "\n",
    "    plt.savefig(dpi=300, fname='reward.png')\n",
    "\n",
    "    print('total reward for evaluation', np.sum(rewards))\n",
    "    return np.sum(rewards)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    P_ID = 0\n",
    "    env = virl.Epidemic(problem_id=P_ID, stochastic=False, noisy=False) # 创建环境\n",
    "    agent = SARSA(actions=list(range(env.action_space.n)))\n",
    "    train(env, agent, 10)\n",
    "    evaluation(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
